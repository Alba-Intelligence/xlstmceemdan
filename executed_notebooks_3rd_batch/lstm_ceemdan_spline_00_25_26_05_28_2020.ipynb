{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008734,
     "end_time": "2020-05-28T03:25:27.280294",
     "exception": false,
     "start_time": "2020-05-28T03:25:27.271560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LSTM-CEEMDAN Spline model\n",
    "\n",
    "## Python requirements\n",
    "\n",
    "```\n",
    "!pip install plotly\n",
    "!pip install cufflinks\n",
    "!pip install chart_studio\n",
    "!pip install ipywidgets\n",
    "!pip install yfinance\n",
    "!pip install EMD-signal\n",
    "!pip install sklearn\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "```\n",
    "\n",
    "## Model\n",
    "\n",
    "The **LSTM-CEEMDAN** works with daily data from equities: open, high, low, close, volume. Hereinafter we are calling these \"features\".\n",
    "\n",
    "### Data processing steps:\n",
    "\n",
    "1. Data gathering\n",
    "\n",
    "We use yfinance package to get the daily ticker history. We obtain a dataframe with one time series for each of the features, for each of the tickers we are working with.\n",
    "\n",
    "Here we are considering the top 10 most liquid tickers in Brazilian B3 exchange in period of **2019-01-01 until 2020-05-01**:\n",
    "\n",
    "**PETR4, VALE3, BOVA11, ITUB4, BBDC4, B3SA3, BBAS3, ABEV3, MGLU3, VVAR3**\n",
    "\n",
    "2. Decomposition\n",
    "\n",
    "We decompose the data with the complete ensemble empirical mode decomposition with adative noise (CEEMDAN) algorithm, using the [PyEMD public package from Dawid Laszuk](github.com/laszukdawid/PyEMD).\n",
    "\n",
    "As a result, we obtain a set of instrinsic mode functions (IMFs) time series for each of the features, for each of the tickers. The number of IMFs resulting from a decomposition may vary, and depends upon CEEMDAN hyperparameters such as scale of added noise, specific series characteristics, and mainly **series length**.\n",
    "\n",
    "3. Data transformation\n",
    "\n",
    "For each of the IMFs series obtained, we tranform the data according to the following equation in order to work with data values between 0 and 1.\n",
    "\n",
    "$$x' = \\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "\n",
    "Where x is any element in the series to be transformed, x' is the equivalent transformed element. The equation also takes into account the maximum and minimum values of the series.\n",
    "\n",
    "4. Windowing\n",
    "\n",
    "Besides the mathematical transformation, each series undergo a vectorial transformation in order to split the data into windows.\n",
    "\n",
    "The process is examplified:\n",
    "\n",
    "```\n",
    "original single dimensional series = [1,2,3,4,5,6,7,8]\n",
    "Splitting into windows of length 4.\n",
    "Windowed data = [[1,2,3,4],[2,3,4,5],[3,4,5,6],[4,5,6,7],[5,6,7,8]]\n",
    "```\n",
    "This way the resulting series will have $n-w+1$ elements, where $n$ is the length of the orginal series and $w$ is the window length.\n",
    "\n",
    "### Processed data availability\n",
    "\n",
    "After the processing method, the final data is available in a dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 3.861297,
     "end_time": "2020-05-28T03:25:31.146323",
     "exception": false,
     "start_time": "2020-05-28T03:25:27.285026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cufflinks as cf\n",
    "import chart_studio.plotly as plotly\n",
    "import plotly.offline\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=True, world_readable=False)\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, LeakyReLU\n",
    "import yfinance as yf\n",
    "from PyEMD import CEEMDAN\n",
    "\n",
    "# iplot layout\n",
    "space =  {\n",
    "            'legend' : {'bgcolor':'#1A1A1C','font':{'color':'#D9D9D9',\"size\":12}},\n",
    "            'paper_bgcolor' : '#1A1A1C',\n",
    "            'plot_bgcolor' : '#1A1A1C',\n",
    "            \"title\" : {\"font\":{\"color\":\"#D9D9D9\"},\"x\":0.5},\n",
    "            'yaxis' : {\n",
    "                'tickfont' : {'color':'#C2C2C2', \"size\":12},\n",
    "                'gridcolor' : '#434343',\n",
    "                'titlefont' : {'color':'#D9D9D9'},\n",
    "                'zerolinecolor' : '#666570',\n",
    "                'showgrid' : True\n",
    "            },\n",
    "            'xaxis' : {\n",
    "                'tickfont' : {'color':'#C2C2C2', \"size\":12},\n",
    "                'gridcolor' : '#434343',\n",
    "                'titlefont' : {'color':'#D9D9D9'},\n",
    "                'zerolinecolor' : '#666570',\n",
    "                'showgrid' : True\n",
    "            },\n",
    "            'titlefont' : {'color':'#D9D9D9'}\n",
    "        }\n",
    "\n",
    "class SplineModel():\n",
    "    def __init__(self,time_series_generator):\n",
    "        self.name = \"SplineModel\"\n",
    "        self.gen = time_series_generator\n",
    "    \n",
    "    def predict(self, x_window, verbose=0):\n",
    "        result = []\n",
    "        x_window = np.squeeze(x_window, axis=0)\n",
    "        last_element_index = x_window.shape[1]-1\n",
    "        series = x_window[:,last_element_index].reshape(-1)\n",
    "        cs = CubicSpline(np.arange(len(series)), series)\n",
    "        next_value = cs(len(series)+1)\n",
    "        result += [next_value]\n",
    "\n",
    "        return np.array(result).reshape(1,-1) # 1,-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 3.87272,
     "end_time": "2020-05-28T03:25:35.024804",
     "exception": false,
     "start_time": "2020-05-28T03:25:31.152084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tickers = [\"PETR4\", \"VALE3\", \"BOVA11\", \"ITUB4\", \"BBDC4\", \"B3SA3\", \"BBAS3\", \"ABEV3\", \"MGLU3\", \"VVAR3\"]\n",
    "\n",
    "tickers = [f\"{ticker}.SA\" for ticker in tickers]\n",
    "\n",
    "start_datetime = datetime(year=2018, month=7, day=20)\n",
    "end_datetime = datetime(year=2019, month=12, day=2)\n",
    "\n",
    "history_data = {ticker.split('.')[0]:yf.download(ticker, start=start_datetime, end=end_datetime).drop(['Close','Open', 'High', 'Low', 'Volume'], axis=1).dropna() for ticker in tickers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 41.935058,
     "end_time": "2020-05-28T03:26:16.969652",
     "exception": false,
     "start_time": "2020-05-28T03:25:35.034594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PETR4] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALE3] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOVA11] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ITUB4] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BBDC4] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B3SA3] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BBAS3] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ABEV3] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MGLU3] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VVAR3] Decomposing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.6 s, sys: 2.69 s, total: 55.3 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ceemdan = CEEMDAN()\n",
    "decomposed_data = {}\n",
    "decomposed_ticker_features_series = {}\n",
    "scalers = {}\n",
    "for ticker in history_data:\n",
    "    # iterating every ticker\n",
    "    print(f'[{ticker}] Decomposing...')\n",
    "    ticker_dataframe = history_data[ticker]\n",
    "    decomposed_ticker_features_series[ticker] = {}\n",
    "    scalers[ticker] = {}\n",
    "    for column in ticker_dataframe.columns:\n",
    "        # iterating evey feature\n",
    "        scaler = MinMaxScaler()\n",
    "        decomposed_ticker_features_series[ticker][column] = {}\n",
    "        series = ticker_dataframe[column].values.reshape(-1,1)\n",
    "        scaler.fit(series)\n",
    "        scalers[ticker][column] = scaler\n",
    "        ticker_feature_time_series = np.frombuffer(scaler.transform(series))\n",
    "        ticker_feature_time_series_imfs = ceemdan(ticker_feature_time_series, max_imf=10)\n",
    "        for i, imf_series in enumerate(ticker_feature_time_series_imfs):\n",
    "            # iterating every IMF \n",
    "            if i < len(ticker_feature_time_series_imfs):\n",
    "                decomposed_ticker_features_series[ticker][column][f'IMF{i+1}'] = imf_series\n",
    "            else:\n",
    "                decomposed_ticker_features_series[ticker][column][f'Rsd'] = imf_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.042265,
     "end_time": "2020-05-28T03:26:17.024059",
     "exception": false,
     "start_time": "2020-05-28T03:26:16.981794",
     "status": "completed"
    },
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Data organisation\n",
    "features_in_order = ['Adj Close']\n",
    "\n",
    "max_window_size = 10\n",
    "windows_sizes_for_imf_level = {\n",
    "    'IMF1': 2,\n",
    "    'IMF2': 2,\n",
    "    'IMF3': 3,\n",
    "    'IMF4': 3,\n",
    "    'IMF5': 4,\n",
    "    'IMF6': 4,\n",
    "    'IMF7': 5,\n",
    "    'IMF8': 5,\n",
    "    'Rsd': 6,\n",
    "    'DEFAULT': 4\n",
    "}\n",
    "\n",
    "# Coupling together the IMFs of the same level for different features to create exogenous input\n",
    "# The number of imfs for each feature decomposition may differ, thus some of the last imfs may not match in number of features\n",
    "series = {}\n",
    "for ticker in decomposed_ticker_features_series:\n",
    "    series[ticker] = {}\n",
    "    for feature in decomposed_ticker_features_series[ticker]:\n",
    "        imfs = pd.DataFrame.from_dict(decomposed_ticker_features_series[ticker][feature])\n",
    "        for imf in imfs:\n",
    "            if imf not in series[ticker]:\n",
    "                series[ticker][imf] = []\n",
    "            _series = imfs[imf].values\n",
    "            _series = _series.reshape((len(_series),1))\n",
    "            series[ticker][imf] += [_series] # reshaping to get into column format\n",
    "\n",
    "dataset = {}\n",
    "# # horizontal stack\n",
    "for ticker in series:\n",
    "    dataset[ticker] = {}\n",
    "    for imf_level in series[ticker]:\n",
    "        dataset[ticker][imf_level] = np.hstack(tuple(series[ticker][imf_level]))\n",
    "\n",
    "# # data set split rates\n",
    "train = 0.7\n",
    "validation = 0.2\n",
    "test = 0.1\n",
    "\n",
    "train_dataset = {}\n",
    "validation_dataset = {}\n",
    "test_dataset = {}\n",
    "\n",
    "train_generators = {}\n",
    "validation_generators = {}\n",
    "test_generators = {}\n",
    "\n",
    "for ticker in dataset:\n",
    "\n",
    "    train_dataset[ticker] = {}\n",
    "    validation_dataset[ticker] = {}\n",
    "    test_dataset[ticker] = {}\n",
    "\n",
    "    train_generators[ticker] = {}\n",
    "    validation_generators[ticker] = {}\n",
    "    test_generators[ticker] = {}\n",
    "\n",
    "    for imf_level in dataset[ticker]:\n",
    "        # splitting data sets according to rates\n",
    "        train_dataset[ticker][imf_level] = dataset[ticker][imf_level][:round(train*dataset[ticker][imf_level].shape[0]),:]\n",
    "        validation_dataset[ticker][imf_level] = dataset[ticker][imf_level][round(train*dataset[ticker][imf_level].shape[0]):round((train+validation)*dataset[ticker][imf_level].shape[0]),:]\n",
    "        test_dataset[ticker][imf_level] = dataset[ticker][imf_level][round((train+validation)*dataset[ticker][imf_level].shape[0]):,:]\n",
    "\n",
    "        if imf_level in windows_sizes_for_imf_level:\n",
    "            window_size = windows_sizes_for_imf_level[imf_level]\n",
    "        else: \n",
    "            window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "        # windowing\n",
    "        train_generators[ticker][imf_level] = TimeseriesGenerator(train_dataset[ticker][imf_level], train_dataset[ticker][imf_level], length=window_size, batch_size=1)\n",
    "        validation_generators[ticker][imf_level] = TimeseriesGenerator(validation_dataset[ticker][imf_level], validation_dataset[ticker][imf_level], length=window_size, batch_size=1)\n",
    "        test_generators[ticker][imf_level] = TimeseriesGenerator(test_dataset[ticker][imf_level], test_dataset[ticker][imf_level], length=window_size, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 23.231276,
     "end_time": "2020-05-28T03:26:40.267947",
     "exception": false,
     "start_time": "2020-05-28T03:26:17.036671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [PETR4][IMF1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [PETR4][IMF2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [PETR4][IMF3]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Model Training\n",
    "\n",
    "models = {}\n",
    "\n",
    "model_epochs = {\n",
    "    'IMF1': 2500,\n",
    "    'IMF2': 2000,\n",
    "    'IMF3': 1500,\n",
    "    'IMF4': 1500,\n",
    "    'IMF5': 1500,\n",
    "    'IMF6': 1200,\n",
    "    'IMF7': 1200,\n",
    "    'IMF8': 1000,\n",
    "    'Rsd': 1000,\n",
    "    'DEFAULT': 1000\n",
    "}\n",
    "\n",
    "imfs_to_predict_with_neural = ['IMF1','IMF2','IMF3']\n",
    "\n",
    "for ticker in train_generators:\n",
    "    models[ticker] = {}\n",
    "    for imf_level in train_generators[ticker]:\n",
    "        if imf_level in imfs_to_predict_with_neural:\n",
    "            print(f'Training model [{ticker}][{imf_level}]')\n",
    "            # Prediction model\n",
    "            model = Sequential()\n",
    "            current_dataset = train_dataset[ticker][imf_level]\n",
    "            n_features = current_dataset.shape[1]\n",
    "            cur_tmp_gen = train_generators[ticker][imf_level]\n",
    "\n",
    "            if imf_level in windows_sizes_for_imf_level:\n",
    "                window_size = windows_sizes_for_imf_level[imf_level]\n",
    "            else: \n",
    "                window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "\n",
    "            model.add(LSTM(128, activation='tanh', return_sequences=True, input_shape=(window_size, n_features)))\n",
    "            model.add(LSTM(64, activation='tanh', input_shape=(window_size, 128)))\n",
    "            model.add(Dense(16))\n",
    "            model.add(Dense(4))\n",
    "            model.add(Dense(n_features))\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "            number_of_epochs = model_epochs[imf_level]\n",
    "            # fit model\n",
    "            model.fit_generator(cur_tmp_gen, steps_per_epoch=1, epochs=number_of_epochs, verbose=0)\n",
    "\n",
    "            models[ticker][imf_level] = model\n",
    "        else:\n",
    "            # Spline prediction model\n",
    "            cur_tmp_gen = train_generators[ticker][imf_level]\n",
    "            model = SplineModel(cur_tmp_gen)\n",
    "            models[ticker][imf_level] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for ticker in models:\n",
    "    results[ticker] = {}\n",
    "\n",
    "    # initializing results dicitionary\n",
    "    for feature in features_in_order:\n",
    "        results[ticker][feature] = {}\n",
    "        for imf_level in models[ticker]:\n",
    "            results[ticker][feature][imf_level] = {\n",
    "                'real_train': [],\n",
    "                'predicted_train': [],\n",
    "                'real_validation': [],\n",
    "                'predicted_validation': [],\n",
    "                'real_test': [],\n",
    "                'predicted_test': [],\n",
    "                'x_axis_train': [],\n",
    "                'x_axis_validation': [],\n",
    "                'x_axis_test': []\n",
    "            }\n",
    "\n",
    "    for imf_level in models[ticker]:\n",
    "        model = models[ticker][imf_level]\n",
    "        \n",
    "        print(f'Predicting: [{ticker}][{imf_level}]')\n",
    "\n",
    "        cur_train_gen = train_generators[ticker][imf_level]\n",
    "        cur_validation_gen = validation_generators[ticker][imf_level]\n",
    "        cur_test_gen = test_generators[ticker][imf_level]\n",
    "\n",
    "        # predicting train\n",
    "        day_counter = 0\n",
    "        for i in range(len(cur_train_gen)):\n",
    "            x, y = cur_train_gen[i]\n",
    "            yhat = model.predict(x, verbose=0)\n",
    "\n",
    "            for j in range(yhat.shape[1]):\n",
    "                results[ticker][features_in_order[j]][imf_level]['real_train'] += [y[:,j][0]]\n",
    "                results[ticker][features_in_order[j]][imf_level]['predicted_train'] += [yhat[:,j][0]]\n",
    "                results[ticker][features_in_order[j]][imf_level]['x_axis_train'] += [day_counter]\n",
    "            day_counter += 1\n",
    "\n",
    "        # predicting validation\n",
    "        for i in range(len(cur_validation_gen)):\n",
    "            x, y = cur_validation_gen[i]\n",
    "            yhat = model.predict(x, verbose=0)\n",
    "\n",
    "            for j in range(yhat.shape[1]):\n",
    "                results[ticker][features_in_order[j]][imf_level]['real_validation'] += [y[:,j][0]]\n",
    "                results[ticker][features_in_order[j]][imf_level]['predicted_validation'] += [yhat[:,j][0]]\n",
    "                results[ticker][features_in_order[j]][imf_level]['x_axis_validation'] += [day_counter]\n",
    "            day_counter += 1\n",
    "\n",
    "        # predicting test\n",
    "        for i in range(len(cur_test_gen)):\n",
    "            x, y = cur_test_gen[i]\n",
    "            yhat = model.predict(x, verbose=0)\n",
    "\n",
    "            for j in range(yhat.shape[1]):\n",
    "                results[ticker][features_in_order[j]][imf_level]['real_test'] += [y[:,j][0]]\n",
    "                results[ticker][features_in_order[j]][imf_level]['predicted_test'] += [yhat[:,j][0]]\n",
    "                results[ticker][features_in_order[j]][imf_level]['x_axis_test'] += [day_counter]\n",
    "            day_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# organizing imf prediction results, concatenating train, validation and test\n",
    "concatenated_results = {}\n",
    "\n",
    "for ticker in results:\n",
    "    concatenated_results[ticker] = {}\n",
    "    for feature in results[ticker]:\n",
    "        concatenated_results[ticker][feature] = {}\n",
    "        for imf_level in results[ticker][feature]:\n",
    "            \n",
    "            df_result = pd.DataFrame.from_dict(results[ticker][feature][imf_level], orient='index').T\n",
    "            df_train = df_result[['real_train','predicted_train','x_axis_train']].set_index('x_axis_train').dropna(axis=0)\n",
    "            df_train.index.name = 'x'\n",
    "            df_validation = df_result[['real_validation','predicted_validation','x_axis_validation']].set_index('x_axis_validation').dropna(axis=0)\n",
    "            df_validation.index.name = 'x'\n",
    "            df_test = df_result[['real_test','predicted_test','x_axis_test']].set_index('x_axis_test').dropna(axis=0)\n",
    "            df_test.index.name = 'x'\n",
    "\n",
    "            df_concatenated = pd.concat([df_train,df_validation,df_test], axis=1)\n",
    "\n",
    "            concatenated_results[ticker][feature][imf_level] = df_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting partial result\n",
    "plot_ticker = 'PETR4'\n",
    "plot_feature = 'Adj Close'\n",
    "plot_imf = 'IMF1'\n",
    "\n",
    "concatenated_results[plot_ticker][plot_feature][plot_imf].iplot(title=f'{plot_ticker} {plot_feature} {plot_imf}', asFigure=True, layout=space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated_results_copy = concatenated_results.copy()\n",
    "accuracies_per_imf_detailed = {}\n",
    "\n",
    "for ticker in concatenated_results_copy:\n",
    "    accuracies_per_imf_detailed[ticker] = {}\n",
    "    for feature in concatenated_results_copy[ticker]:\n",
    "        # we are predicting only Adj Close, so we don't need to specify which feature we are predicting here\n",
    "        # accuracies_per_imf_detailed[ticker][feature] = {}\n",
    "        for imf_level in concatenated_results_copy[ticker][feature]:\n",
    "            real_test = concatenated_results_copy[ticker][feature][imf_level]['real_test'].values\n",
    "            predicted_test = concatenated_results_copy[ticker][feature][imf_level]['predicted_test'].values\n",
    "            real_validation = concatenated_results_copy[ticker][feature][imf_level]['real_validation'].values\n",
    "            predicted_validation = concatenated_results_copy[ticker][feature][imf_level]['predicted_validation'].values\n",
    "            real_train = concatenated_results_copy[ticker][feature][imf_level]['real_train'].values\n",
    "            predicted_train = concatenated_results_copy[ticker][feature][imf_level]['predicted_train'].values\n",
    "\n",
    "            # removing offset nan\n",
    "            real_test = real_test[~np.isnan(real_test)]\n",
    "            predicted_test = predicted_test[~np.isnan(predicted_test)]\n",
    "            real_validation = real_validation[~np.isnan(real_validation)]\n",
    "            predicted_validation = predicted_validation[~np.isnan(predicted_validation)]\n",
    "            real_train = real_train[~np.isnan(real_train)]\n",
    "            predicted_train = predicted_train[~np.isnan(predicted_train)]\n",
    "\n",
    "            accuracies_per_imf_detailed[ticker][f'mse_test_{imf_level}'] = mean_squared_error(real_train,predicted_train)\n",
    "            accuracies_per_imf_detailed[ticker][f'mape_test_{imf_level}'] = np.mean(np.abs((real_train - predicted_train) / real_train)) * 100\n",
    "\n",
    "            # we sre only interested in the test accuracy here, so no need for specifying train and validation accuracies\n",
    "            # accuracies_per_imf_detailed[ticker] = {\n",
    "            #     'mse_train':mean_squared_error(real_test,predicted_test),\n",
    "            #     'mse_validation':mean_squared_error(real_validation,predicted_validation),\n",
    "            #     'mse_test':mean_squared_error(real_train,predicted_train),\n",
    "            #     'mape_train':np.mean(np.abs((real_test - predicted_test) / real_test)) * 100,\n",
    "            #     'mape_validation':np.mean(np.abs((real_validation - predicted_validation) / real_validation)) * 100,\n",
    "            #     'mape_test':np.mean(np.abs((real_train - predicted_train) / real_train)) * 100,\n",
    "            # }\n",
    "\n",
    "df_accuracies_by_imfs = pd.DataFrame.from_dict(accuracies_per_imf_detailed)\n",
    "df_accuracies_by_imfs.to_csv(f'exp_records_accuracy_per_imf/lstm_ceemdan_spline_per_imf_{datetime.now().strftime(\"%H_%M_%S_%m_%d_%Y\")}.csv', sep=',', encoding='utf-8')\n",
    "df_accuracies_by_imfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# recomposing prediction by arithmetically adding the IMF curves\n",
    "\n",
    "final_prediction_results = {}\n",
    "max_window_size = 10\n",
    "\n",
    "for ticker in concatenated_results:\n",
    "    final_prediction_results[ticker] = {}\n",
    "    for feature in concatenated_results[ticker]:\n",
    "        addition_train = None\n",
    "        addition_validation = None\n",
    "        addition_test = None\n",
    "\n",
    "        addition_real_train = None\n",
    "        addition_real_validation = None\n",
    "        addition_real_test = None\n",
    "\n",
    "        # recomposing predictions\n",
    "        for imf_level in concatenated_results[ticker][feature]:\n",
    "            # adding test\n",
    "            can_sum = True\n",
    "            if addition_test is None:\n",
    "                addition_test = concatenated_results[ticker][feature][imf_level]['predicted_test'].values\n",
    "            else:\n",
    "                np_array_to_be_added = concatenated_results[ticker][feature][imf_level]['predicted_test'].values\n",
    "                cur_length = addition_test.shape[0]\n",
    "                next_np_array_length = np_array_to_be_added.shape[0]\n",
    "                if cur_length < next_np_array_length:\n",
    "                    if next_np_array_length-cur_length < max_window_size:\n",
    "                        np_array_to_be_added = np_array_to_be_added[next_np_array_length-cur_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                else: \n",
    "                    if cur_length-next_np_array_length < max_window_size:\n",
    "                        addition_test = addition_test[cur_length-next_np_array_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                \n",
    "                if can_sum:\n",
    "                    addition_test = np.add(addition_test,np_array_to_be_added)\n",
    "\n",
    "        for imf_level in concatenated_results[ticker][feature]:\n",
    "            # adding train\n",
    "            can_sum = True\n",
    "            if addition_train is None:\n",
    "                addition_train = concatenated_results[ticker][feature][imf_level]['predicted_train'].values\n",
    "            else:\n",
    "                np_array_to_be_added = concatenated_results[ticker][feature][imf_level]['predicted_train'].values\n",
    "                cur_length = addition_train.shape[0]\n",
    "                next_np_array_length = np_array_to_be_added.shape[0]\n",
    "                if cur_length < next_np_array_length:\n",
    "                    if next_np_array_length-cur_length < max_window_size:\n",
    "                        np_array_to_be_added = np_array_to_be_added[next_np_array_length-cur_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                else: \n",
    "                    if cur_length-next_np_array_length < max_window_size:\n",
    "                        addition_train = addition_train[cur_length-next_np_array_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                \n",
    "                if can_sum:\n",
    "                    addition_train = np.add(addition_train,np_array_to_be_added)\n",
    "\n",
    "        for imf_level in concatenated_results[ticker][feature]:\n",
    "            # adding validation\n",
    "            can_sum = True\n",
    "            if addition_validation is None:\n",
    "                addition_validation = concatenated_results[ticker][feature][imf_level]['predicted_validation'].values\n",
    "            else:\n",
    "                np_array_to_be_added = concatenated_results[ticker][feature][imf_level]['predicted_validation'].values\n",
    "                cur_length = addition_validation.shape[0]\n",
    "                next_np_array_length = np_array_to_be_added.shape[0]\n",
    "                if cur_length < next_np_array_length:\n",
    "                    if next_np_array_length-cur_length < max_window_size:\n",
    "                        np_array_to_be_added = np_array_to_be_added[next_np_array_length-cur_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                else: \n",
    "                    if cur_length-next_np_array_length < max_window_size:\n",
    "                        addition_validation = addition_validation[cur_length-next_np_array_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                \n",
    "                if can_sum:\n",
    "                    addition_validation = np.add(addition_validation,np_array_to_be_added)\n",
    "\n",
    "        # recomposing real\n",
    "        for imf_level in concatenated_results[ticker][feature]:\n",
    "            # adding test\n",
    "            can_sum = True\n",
    "            if addition_real_test is None:\n",
    "                addition_real_test = concatenated_results[ticker][feature][imf_level]['real_test'].values\n",
    "            else:\n",
    "                np_array_to_be_added = concatenated_results[ticker][feature][imf_level]['real_test'].values\n",
    "                cur_length = addition_real_test.shape[0]\n",
    "                next_np_array_length = np_array_to_be_added.shape[0]\n",
    "                if cur_length < next_np_array_length:\n",
    "                    if next_np_array_length-cur_length < max_window_size:\n",
    "                        np_array_to_be_added = np_array_to_be_added[next_np_array_length-cur_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                else: \n",
    "                    if cur_length-next_np_array_length < max_window_size:\n",
    "                        addition_real_test = addition_real_test[cur_length-next_np_array_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                \n",
    "                if can_sum:\n",
    "                    addition_real_test = np.add(addition_real_test,np_array_to_be_added)\n",
    "\n",
    "        for imf_level in concatenated_results[ticker][feature]:\n",
    "            # adding train\n",
    "            can_sum = True\n",
    "            if addition_real_train is None:\n",
    "                addition_real_train = concatenated_results[ticker][feature][imf_level]['real_train'].values\n",
    "            else:\n",
    "                np_array_to_be_added = concatenated_results[ticker][feature][imf_level]['real_train'].values\n",
    "                cur_length = addition_real_train.shape[0]\n",
    "                next_np_array_length = np_array_to_be_added.shape[0]\n",
    "                if cur_length < next_np_array_length:\n",
    "                    if next_np_array_length-cur_length < max_window_size:\n",
    "                        np_array_to_be_added = np_array_to_be_added[next_np_array_length-cur_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                else: \n",
    "                    if cur_length-next_np_array_length < max_window_size:\n",
    "                        addition_real_train = addition_real_train[cur_length-next_np_array_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                \n",
    "                if can_sum:\n",
    "                    addition_real_train = np.add(addition_real_train,np_array_to_be_added)\n",
    "\n",
    "        for imf_level in concatenated_results[ticker][feature]:\n",
    "            # adding validation\n",
    "            can_sum = True\n",
    "            if addition_real_validation is None:\n",
    "                addition_real_validation = concatenated_results[ticker][feature][imf_level]['real_validation'].values\n",
    "            else:\n",
    "                np_array_to_be_added = concatenated_results[ticker][feature][imf_level]['real_validation'].values\n",
    "                cur_length = addition_real_validation.shape[0]\n",
    "                next_np_array_length = np_array_to_be_added.shape[0]\n",
    "                if cur_length < next_np_array_length:\n",
    "                    if next_np_array_length-cur_length < max_window_size:\n",
    "                        np_array_to_be_added = np_array_to_be_added[next_np_array_length-cur_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                else: \n",
    "                    if cur_length-next_np_array_length < max_window_size:\n",
    "                        addition_real_validation = addition_real_validation[cur_length-next_np_array_length:]\n",
    "                    else:\n",
    "                        can_sum = False\n",
    "                \n",
    "                if can_sum:\n",
    "                    addition_real_validation = np.add(addition_real_validation,np_array_to_be_added)\n",
    "        \n",
    "        scaler = scalers[ticker][feature]\n",
    "\n",
    "        final_prediction_results[ticker][feature] = {\n",
    "            'train_predicted': scaler.inverse_transform(addition_train.reshape(-1,1)).reshape(-1),\n",
    "            'validation_predicted': scaler.inverse_transform(addition_validation.reshape(-1,1)).reshape(-1),\n",
    "            'test_predicted': scaler.inverse_transform(addition_test.reshape(-1,1)).reshape(-1),\n",
    "            'train_real': scaler.inverse_transform(addition_real_train.reshape(-1,1)).reshape(-1),\n",
    "            'validation_real': scaler.inverse_transform(addition_real_validation.reshape(-1,1)).reshape(-1),\n",
    "            'test_real': scaler.inverse_transform(addition_real_test.reshape(-1,1)).reshape(-1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting final result\n",
    "plot_ticker = 'PETR4'\n",
    "plot_feature = 'Adj Close'\n",
    "\n",
    "pd.DataFrame.from_dict(final_prediction_results[plot_ticker][plot_feature]).iplot(title=f'{plot_ticker} {plot_feature} {plot_imf}', layout=space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculating accuracy metrics\n",
    "\n",
    "adj_close_accuracies = {}\n",
    "accuracies_detailed = {}\n",
    "\n",
    "for ticker in final_prediction_results:\n",
    "    adj_close_accuracies[ticker] = {}\n",
    "    accuracies_detailed[ticker] = {}\n",
    "    for feature in final_prediction_results[ticker]:\n",
    "\n",
    "        y_train = final_prediction_results[ticker][feature]['train_predicted'][~np.isnan(final_prediction_results[ticker][feature]['train_predicted'])]\n",
    "        yhat_train = final_prediction_results[ticker][feature]['train_real'][~np.isnan(final_prediction_results[ticker][feature]['train_real'])]\n",
    "\n",
    "        y_validation = final_prediction_results[ticker][feature]['validation_predicted'][~np.isnan(final_prediction_results[ticker][feature]['validation_predicted'])]\n",
    "        yhat_validation = final_prediction_results[ticker][feature]['validation_real'][~np.isnan(final_prediction_results[ticker][feature]['validation_real'])]\n",
    "\n",
    "        y_test = final_prediction_results[ticker][feature]['test_predicted'][~np.isnan(final_prediction_results[ticker][feature]['test_predicted'])]\n",
    "        yhat_test = final_prediction_results[ticker][feature]['test_real'][~np.isnan(final_prediction_results[ticker][feature]['test_real'])]\n",
    "        accuracies_detailed[ticker][feature] = {\n",
    "            'mse':{\n",
    "                'train':mean_squared_error(y_train,yhat_train),\n",
    "                'validation':mean_squared_error(y_validation,yhat_validation),\n",
    "                'test':mean_squared_error(y_test,yhat_test),\n",
    "            },\n",
    "            'mape':{\n",
    "                'train':np.mean(np.abs((y_train - yhat_train) / y_train)) * 100,\n",
    "                'validation':np.mean(np.abs((y_validation - yhat_validation) / y_validation)) * 100,\n",
    "                'test':np.mean(np.abs((y_test - yhat_test) / y_test)) * 100,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if feature == 'Adj Close':\n",
    "            adj_close_accuracies[ticker] = {\n",
    "                'mse': mean_squared_error(y_test,yhat_test),\n",
    "                'mape': np.mean(np.abs((y_validation - yhat_validation) / y_validation)) * 100\n",
    "            }\n",
    "\n",
    "# pd.DataFrame.from_dict(accuracies_detailed[plot_ticker][plot_feature])\n",
    "df_close_accuracies = pd.DataFrame.from_dict(adj_close_accuracies).T\n",
    "df_close_accuracies.to_csv(f'exp_records/lstm_ceemdan_spline_{datetime.now().strftime(\"%H_%M_%S_%m_%d_%Y\")}.csv', sep=',', encoding='utf-8')\n",
    "df_close_accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python37464bit1abd2d5f941841b49c9f1b2b4252e81e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "papermill": {
   "duration": 74.753635,
   "end_time": "2020-05-28T03:26:40.863425",
   "environment_variables": {},
   "exception": null,
   "input_path": "./lstm_ceemdan_spline.ipynb",
   "output_path": "./executed_notebooks/lstm_ceemdan_spline_00_25_26_05_28_2020.ipynb",
   "parameters": {},
   "start_time": "2020-05-28T03:25:26.109790",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}